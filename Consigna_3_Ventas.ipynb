{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook de ejemplo uso de pySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisitos y consideraciones\n",
    "- Instalar las extensiones de Python y Jupyter en Visual Studio Code.\n",
    "- Tener el entorno Python configurado: \n",
    "    * Conda env con Python 3.7 (máxima compatible con pyspark:2.4) y librerías instaladas (en principio pyspark:2.4 y sus dependencias). \n",
    "    * Se encuentra disponible un entorno conda en la ruta */data/DWRM/pyenv/.conda/envs/spark24* del servidor *dpyserver02*.\n",
    "- Activar el entorno en la terminal ejecutando: *conda activate spark24*. Si esto no funciona, chequear que existan las siguientes variables de entorno en el perfil de su usuario (archivos *$HOME/.profile* y *$HOME/.bash_profile*) y abrir una nueva terminal:\n",
    "    ```\n",
    "        export CONDA_PKGS_DIRS=/data/DWRM/pyenv/.conda/pkgs\n",
    "        export CONDA_ENVS_PATH=/data/DWRM/pyenv/.conda/envs\n",
    "    ```\n",
    "- Tener el kernel de Jupyter instalado. Se instala ejecutando:\n",
    "    ```\n",
    "        python -m ipykernel install --user --name spark24 --display-name \"Python_spark2.4_kernel\"\n",
    "    ```\n",
    "- Abrir la notebook y elegir el kernel desde Visual Studio Code en la parte superior derecha según el display-name que se eligió en el paso anterior, en caso de no verlo .\n",
    "- Para conectarse al clúster pedir previamente el ticket de Kerberos ejecutando el comando *kinit* desde la terminal del VS Code. Cuestiones a tener en cuenta: \n",
    "    * El comando por defecto los identifica con el ID de usuario de sistema operativo (legajo) y al pedirles la contraseña deberían ingresar la que corresponde a la cuenta de LDAP.\n",
    "    * Los tickets (token) tienen un vencimiento, duran apróximadamente 24hs y luego de ese tiempo deberían generar uno nuevo con el comando *kinit*. Cada vez que hagan esto van a tener que reiniciar el kernel de la notebook para que lo reconozca.\n",
    "    * **Extra**: Una opción alternativa para pedir el ticket es usar un archivo de clave encriptado o *keytab*, éste lo deberían actualizar cada vez que hagan un cambio de contraseña. Para crearlo o modificarlo:\n",
    "        1. Abrir la consola de administración de keytabs:\n",
    "        ```\n",
    "            ktutil\n",
    "        ```\n",
    "        2. Agregamos la cuenta que vamos a usar para el keytab:\n",
    "        ```\n",
    "            addent -password -p {idUsuario}@BGCMZ.BANCOGALICIA.COM.AR -k 1 -e RC4-HMAC\n",
    "        ```\n",
    "        3. Al presionar enter nos va a pedir la contraseña:\n",
    "        ```\n",
    "            - enter password for username -\n",
    "        ```\n",
    "        4. Usamos la siguiente instrucción para guardar el keytab:\n",
    "        ```\n",
    "            wkt {idUsuario}.keytab\n",
    "        ```\n",
    "        > *Nota: si no especificamos una ruta se va a usar el directorio actual, por seguridad lo recomendable es que quede en el home de su usurio ($HOME), más allá de que el archivo se va a generar con permisos para que solo pueda leerse y modificarse con su cuenta.*\n",
    "        5. Salimos de la consola:\n",
    "        ```\n",
    "            q\n",
    "        ```\n",
    "        6. Probamos de solicitar el ticket usando el keytab:\n",
    "        ```\n",
    "            kinit -kt $HOME/{idUsuario}.keytab {idUsuario}\n",
    "        ```\n",
    "- Para trabajar desde un script, recordar activar el entorno previamente a ejecutarlo y, si van a usar Hadoop, tener un ticket de Kerberos activo y prestarle atención, tanto a las variables de entorno, como a los parámetros que se le van a pasar a la sesión de Spark (mencionados a continuación).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las librerias y paquetes a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución en el cluster\n",
    "Creando una sesión que utilice **yarn** como master, las ejecuciones las realizará el cluster Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de variables de entorno\n",
    "Primero debemos definir las siguientes variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = '/data/DWRM/pyenv/spark/spark-2.4.0-bin-hadoop2.7'\n",
    "os.environ['HADOOP_CONF_DIR'] = '/data/DWRM/pyenv/conf/hadoop'\n",
    "os.environ['PATH'] = os.environ['SPARK_HOME'] + '/bin' + ':' + os.environ['PATH']\n",
    "os.environ['JAVA_HOME'] = '/data/jdk1.8.0_172'\n",
    "os.environ['HOSTALIASES'] = '/data/DWRM/pyenv/conf/hadoop/hosts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luego construimos la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-24 11:44:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-08-24 11:44:11 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2022-08-24 11:44:12 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-08-24 11:44:25 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.input.format.excludes does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist\n",
      "2022-08-24 11:44:34 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master('yarn') \\\n",
    ".appName('Test hive') \\\n",
    ".config('spark.hadoop.hive.exec.stagingdir', '/tmp/.hive-staging') \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe seleccionar BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vector.serde.deserialize does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.adaptor.usage.mode does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.use.vectorized.input.format does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.vectorized.input.format.excludes does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.bucketing does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.type.safety does not exist\n",
      "2022-08-24 11:47:13 WARN  HiveConf:2753 - HiveConf of name hive.strict.checks.cartesian.product does not exist\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+------------+--------------------+-----------+\n",
      "|    database|           tableName|isTemporary|\n",
      "+------------+--------------------+-----------+\n",
      "|academia_lnd|aalg_movimientos_atm|      false|\n",
      "|academia_lnd|  l0303208_empleados|      false|\n",
      "|academia_lnd|  l0330272_empleados|      false|\n",
      "|academia_lnd|       l0330272_fact|      false|\n",
      "|academia_lnd|  l0330272_productos|      false|\n",
      "|academia_lnd| l0330272_sucursales|      false|\n",
      "|academia_lnd|  l0338923_empleados|      false|\n",
      "|academia_lnd|       l0338923_fact|      false|\n",
      "|academia_lnd|  l0338923_productos|      false|\n",
      "|academia_lnd| l0338923_sucursales|      false|\n",
      "|academia_lnd|  l0339202_empleados|      false|\n",
      "|academia_lnd|       l0339202_fact|      false|\n",
      "|academia_lnd|l0339202_fact_ventas|      false|\n",
      "|academia_lnd| l0339202_sucursales|      false|\n",
      "|academia_lnd|  l0501604_empleados|      false|\n",
      "|academia_lnd|       l0501604_fact|      false|\n",
      "|academia_lnd|  l0501604_productos|      false|\n",
      "|academia_lnd| l0501604_sucursales|      false|\n",
      "|academia_lnd|l0505013_dim_vend...|      false|\n",
      "|academia_lnd|  l0505013_empleados|      false|\n",
      "+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('use academia_lnd').show()\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se cargan las tablas en Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+--------+\n",
      "|     fecha| sku|vendedor|cantidad|\n",
      "+----------+----+--------+--------+\n",
      "| timestamp|null|    null|    null|\n",
      "| 11/1/2019|  63|      42|       5|\n",
      "| 16/4/2019| 126|      32|       2|\n",
      "|  8/3/2019|   6|      57|       8|\n",
      "| 11/4/2019|  72|      41|       2|\n",
      "|31/12/2019|  37|      50|       9|\n",
      "|  2/7/2019| 117|      37|       6|\n",
      "| 25/9/2019|  44|      31|       6|\n",
      "| 28/5/2019|  65|      21|       6|\n",
      "| 22/9/2019|  36|      17|       6|\n",
      "+----------+----+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+---------------+\n",
      "|id_producto|familia|    nombre|precio_unitario|\n",
      "+-----------+-------+----------+---------------+\n",
      "|       null|familia|    nombre|           null|\n",
      "|          1|  Leche|     Leche|             62|\n",
      "|          2|  Leche|      Nido|            177|\n",
      "|          3|  Leche|      Klim|             36|\n",
      "|          4|  Leche|   Nan Pro|            156|\n",
      "|          5|  Leche| Nestogeno|            134|\n",
      "|          6|  Leche|La lechera|             68|\n",
      "|          7|  Leche| Canprolac|             36|\n",
      "|          8|  Leche|Sativa 2,3|             22|\n",
      "|          9|  Leche|    Nidina|            247|\n",
      "+-----------+-------+----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+--------------------+--------------------+\n",
      "|id_sucursal|              nombre|                tipo|\n",
      "+-----------+--------------------+--------------------+\n",
      "|       null|              nombre|                tipo|\n",
      "|          1|      Éxito Aranjuez|        Supermercado|\n",
      "|          2|      Éxito Aventura|Comercio de cercania|\n",
      "|          3|         Éxito Belén|        Supermercado|\n",
      "|          4|      Éxito Colombia|        Hipermercado|\n",
      "|          5|      Éxito Del Este|              Vecino|\n",
      "|          6|      Éxito Envigado|        Hipermercado|\n",
      "|          7|Éxito Envigado Ce...|        Supermercado|\n",
      "|          8|      Éxito Gran Vía|        Supermercado|\n",
      "|          9|  Éxito Indiana Mall|Comercio de cercania|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+--------+----------------+\n",
      "|id_vendedor|sucursal|          nombre|\n",
      "+-----------+--------+----------------+\n",
      "|       null|    null|          nombre|\n",
      "|          1|      14| Maria Rodríguez|\n",
      "|          2|       1|      Juan Gómez|\n",
      "|          3|       4|   Jose González|\n",
      "|          4|       1| Carlos Martínez|\n",
      "|          5|       1|    Jorge García|\n",
      "|          6|       6|  Luis Fernández|\n",
      "|          7|       7|Miguel Rodríguez|\n",
      "|          8|       4|    Ana González|\n",
      "|          9|       8|  Hector  García|\n",
      "+-----------+--------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_df = spark.table(\"academia_lnd.l0507199_fact\")\n",
    "productos_df = spark.table(\"academia_lnd.l0507199_productos\")\n",
    "sucursales_df = spark.table(\"academia_lnd.l0507199_sucursales\")\n",
    "empleados_df = spark.table(\"academia_lnd.l0507199_empleados\")\n",
    "fact_df.show(10)\n",
    "productos_df.show(10)\n",
    "sucursales_df.show(10)\n",
    "empleados_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se tranforman Dataframe a vistas temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados_df.createOrReplaceTempView('empleados')\n",
    "fact_df.createOrReplaceTempView('fact')\n",
    "productos_df.createOrReplaceTempView('productos')\n",
    "sucursales_df.createOrReplaceTempView('sucursales')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Se generan tablones para reporteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tablon1_df = spark.sql('''select a.familia, SUM(b.cantidad * a.precio_unitario) AS Monto_Facturado from productos a inner join fact b on a.id_producto = b.sku group by a.familia''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|             familia|Monto_Facturado|\n",
      "+--------------------+---------------+\n",
      "|             Helados|        6809166|\n",
      "|            Papillas|        3044895|\n",
      "|               Leche|        8478184|\n",
      "|            Cereales|       15703271|\n",
      "|                Café|        5799239|\n",
      "|Barritas de Cereales|        3302855|\n",
      "|          Culinarios|        2989699|\n",
      "|          Chocolates|       13582196|\n",
      "|Comidas para anim...|        4070014|\n",
      "|       Otras bebidas|        6275111|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tablon1_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tablon2_df = spark.sql('''SELECT \n",
    "a.sku as id_producto,\n",
    "a.fecha as cd_periodo,\n",
    "b.nombre as nombre_producto,\n",
    "b.precio_unitario,\n",
    "c.id_sucursal,\n",
    "c.nombre as nombre_sucursal,\n",
    "sum(a.cantidad) as total_ventas\n",
    "FROM fact a\n",
    "join productos b on a.sku=b.id_producto\n",
    "join empleados d on a.vendedor=d.id_vendedor\n",
    "join sucursales c on d.sucursal=c.id_sucursal\n",
    "group by a.sku, a.fecha, b.nombre, b.precio_unitario, c.id_sucursal, c.nombre''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------------+---------------+-----------+----------------+------------+\n",
      "|id_producto|cd_periodo|   nombre_producto|precio_unitario|id_sucursal| nombre_sucursal|total_ventas|\n",
      "+-----------+----------+------------------+---------------+-----------+----------------+------------+\n",
      "|         84| 24/2/2019|          Pela-Pop|             98|          3|     Éxito Belén|           4|\n",
      "|         88|15/11/2019|             Maggi|            162|          1|  Éxito Aranjuez|           4|\n",
      "|         95| 12/3/2019|    Nestlé Postres|            216|          8|  Éxito Gran Vía|           1|\n",
      "|         46| 10/7/2019|   Zucosos (Chile)|            174|         14|     Éxito La 33|           9|\n",
      "|         91| 23/2/2019|           Litoral|            216|         11|Éxito La Central|           1|\n",
      "|         94|18/10/2019|  Nestlé Extrafino|             65|          6|  Éxito Envigado|           8|\n",
      "|         28|21/12/2019|Fitness Choc White|            176|          1|  Éxito Aranjuez|           1|\n",
      "|         95|  1/6/2019|    Nestlé Postres|            216|         12|    Éxito Itagüí|          12|\n",
      "|         93| 25/9/2019|       After Eight|            148|          2|  Éxito Aventura|          10|\n",
      "|         56| 15/8/2019|           Nescafé|            172|         13|     Éxito Junin|           3|\n",
      "+-----------+----------+------------------+---------------+-----------+----------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Tablon2_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Guardamos el resultado en HDFS sobrescribiendo los datos\n",
    "Al guardar un DataFrame podemos usar distintos [modos](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes) de escritura (errorifexists, append, overwrite e ignore), formatos (los más conocidos: json, parquet, orc, csv y text) y [opciones extra](https://dbmstutorials.com/pyspark/spark-read-write-dataframe-options.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Tablon1_df.coalesce(1).write.mode(\"overwrite\").csv(\"hdfs://GALICIAHADOOP/galicia/d/landing_files/academia_de/Squad_Financiera_Tablon1/\")\n",
    "Tablon2_df.coalesce(1).write.mode(\"overwrite\").csv(\"hdfs://GALICIAHADOOP/galicia/d/landing_files/academia_de/Squad_Financiera_Tablon2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cerramos la sesión\n",
    "Como estamos trabajando de manera interactiva, debemos cerrar explícitamente la sesión para no dejar el Job corriendo en YARN y liberar los recursos asignados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ab8922e9d0b393247ace4d614b2395f918dd4184e0936032afcdbc381bbdea0"
  },
  "kernelspec": {
   "display_name": "Python_spark24_academia\n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
